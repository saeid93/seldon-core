{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Spacy Reddit Text Classification Example\n",
    "\n",
    "In this example we will be buiding a text classifier using the reddit content moderation dataset.\n",
    "\n",
    "For this, we will be using SpaCy for the word tokenization and lemmatization. \n",
    "\n",
    "The classification will be done with a Logistic Regression binary classifier.\n",
    "For more information please visit: https://towardsdatascience.com/real-time-stream-processing-for-machine-learning-at-scale-with-spacy-kafka-seldon-core-6360f2fedbe\n",
    "\n",
    "The steps in this tutorial include:\n",
    "\n",
    "1) Train and build your NLP model\n",
    "\n",
    "2) Build your containerized model\n",
    "\n",
    "3) Test your model as a docker container\n",
    "\n",
    "4) Run Seldon in your kubernetes cluster\n",
    "\n",
    "5) Deploy your model with Seldon\n",
    "\n",
    "6) Interact with your model through API\n",
    "\n",
    "7) Clean your environment\n",
    "\n",
    "\n",
    "## Before you start\n",
    "Make sure you install the following dependencies, as they are critical for this example to work:\n",
    "\n",
    "* Helm v3.0.0+\n",
    "* A Kubernetes cluster running v1.13 or above (minkube / docker-for-windows work well if enough RAM)\n",
    "* kubectl v1.14+\n",
    "* Python 3.6+\n",
    "* Python DEV requirements (we'll install them below)\n",
    "\n",
    "Let's get started! ðŸš€ðŸ”¥\n",
    "\n",
    "## 1) Train and build your NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "scikit-learn>=0.23.2\n",
    "spacy==2.3.2\n",
    "dill==0.3.2\n",
    "pandas==1.1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn>=0.23.2 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (0.24.1)\n",
      "Collecting spacy==2.3.2\n",
      "  Downloading spacy-2.3.2-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.9 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill==0.3.2\n",
      "  Downloading dill-0.3.2.zip (177 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 177 kB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==1.1.1\n",
      "  Downloading pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.5 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from pandas==1.1.1->-r requirements.txt (line 4)) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from pandas==1.1.1->-r requirements.txt (line 4)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from pandas==1.1.1->-r requirements.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (2.25.1)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184 kB 19.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp37-cp37m-manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (126 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126 kB 25.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (52.0.0.post20210125)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (20 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc==7.4.1\n",
      "  Downloading thinc-7.4.1-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1 MB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.7 MB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from scikit-learn>=0.23.2->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from scikit-learn>=0.23.2->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from scikit-learn>=0.23.2->-r requirements.txt (line 1)) (1.6.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.2->-r requirements.txt (line 2)) (3.7.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.2->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.3.2->-r requirements.txt (line 2)) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.1.1->-r requirements.txt (line 4)) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (4.0.0)\n",
      "Building wheels for collected packages: dill\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78911 sha256=bf7905b0c7ccd22cf2a306704d5f408f62c547aa74f023367fee8a77634c4f53\n",
      "  Stored in directory: /home/nadine/.cache/pip/wheels/72/6b/d5/5548aa1b73b8c3d176ea13f9f92066b02e82141549d90e2100\n",
      "Successfully built dill\n",
      "Installing collected packages: murmurhash, cymem, wasabi, tqdm, srsly, preshed, plac, catalogue, blis, thinc, spacy, pandas, dill\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.0\n",
      "    Uninstalling pandas-1.1.0:\n",
      "      Successfully uninstalled pandas-1.1.0\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.5 dill-0.3.2 murmurhash-1.0.5 pandas-1.1.1 plac-1.1.3 preshed-3.0.5 spacy-2.3.2 srsly-1.0.5 thinc-7.4.1 tqdm-4.59.0 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.0 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.59.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.20.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nadine/miniconda3/envs/core/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from seldon_core.seldon_client import SeldonClient\n",
    "import dill\n",
    "import sys, os\n",
    "\n",
    "# This import may take a while as it will download the Spacy ENGLISH model\n",
    "from ml_utils import CleanTextTransformer, SpacyTokenTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_idx</th>\n",
       "      <th>parent_idx</th>\n",
       "      <th>body</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8756</td>\n",
       "      <td>8877</td>\n",
       "      <td>Always be wary of news articles that cite unpu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7330</td>\n",
       "      <td>7432</td>\n",
       "      <td>The problem I have with this is that the artic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15711</td>\n",
       "      <td>15944</td>\n",
       "      <td>This is indicative of a typical power law, and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1604</td>\n",
       "      <td>1625</td>\n",
       "      <td>This doesn't make sense. Chess obviously trans...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13327</td>\n",
       "      <td>13520</td>\n",
       "      <td>1. I dispute that gene engineering is burdenso...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prev_idx  parent_idx                                               body  \\\n",
       "0      8756        8877  Always be wary of news articles that cite unpu...   \n",
       "1      7330        7432  The problem I have with this is that the artic...   \n",
       "2     15711       15944  This is indicative of a typical power law, and...   \n",
       "3      1604        1625  This doesn't make sense. Chess obviously trans...   \n",
       "4     13327       13520  1. I dispute that gene engineering is burdenso...   \n",
       "\n",
       "   removed  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cols = [\"prev_idx\", \"parent_idx\", \"body\", \"removed\"]\n",
    "\n",
    "TEXT_COLUMN = \"body\" \n",
    "CLEAN_COLUMN = \"clean_body\"\n",
    "TOKEN_COLUMN = \"token_body\"\n",
    "\n",
    "# Downloading the 50k reddit dataset of moderated comments\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/axsauze/reddit-classification-exploration/master/data/reddit_train.csv\", \n",
    "                         names=df_cols, skiprows=1, encoding=\"ISO-8859-1\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how many examples we have of each class\n",
    "df[\"removed\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"body\"].values\n",
    "y = df[\"removed\"].values\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, \n",
    "    stratify=y, \n",
    "    random_state=42, \n",
    "    test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "clean_text_transformer = CleanTextTransformer()\n",
    "x_train_clean = clean_text_transformer.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and get the lemmas\n",
    "spacy_tokenizer = SpacyTokenTransformer()\n",
    "x_train_tokenized = spacy_tokenizer.transform(x_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=10000, ngram_range=(1, 3),\n",
       "                preprocessor=<function <lambda> at 0x7fe61e82f680>,\n",
       "                token_pattern=None,\n",
       "                tokenizer=<function <lambda> at 0x7fe61e82f5f0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    preprocessor=lambda x: x, \n",
    "    tokenizer=lambda x: x, \n",
    "    token_pattern=None,\n",
    "    ngram_range=(1, 3))\n",
    "\n",
    "tfidf_vectorizer.fit(x_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our tokens to tfidf vectors\n",
    "x_train_tfidf = tfidf_vectorizer.transform(x_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, solver='sag')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train logistic regression classifier\n",
    "lr = LogisticRegression(C=0.1, solver='sag')\n",
    "lr.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the models we'll deploy\n",
    "with open('tfidf_vectorizer.model', 'wb') as model_file:\n",
    "    dill.dump(tfidf_vectorizer, model_file)\n",
    "with open('lr.model', 'wb') as model_file:\n",
    "    dill.dump(lr, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Build your containerized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting RedditClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile RedditClassifier.py\n",
    "import dill\n",
    "\n",
    "from ml_utils import CleanTextTransformer, SpacyTokenTransformer\n",
    "\n",
    "\n",
    "class RedditClassifier(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self._clean_text_transformer = CleanTextTransformer()\n",
    "        self._spacy_tokenizer = SpacyTokenTransformer()\n",
    "\n",
    "        with open(\"tfidf_vectorizer.model\", \"rb\") as model_file:\n",
    "            self._tfidf_vectorizer = dill.load(model_file)\n",
    "\n",
    "        with open(\"lr.model\", \"rb\") as model_file:\n",
    "            self._lr_model = dill.load(model_file)\n",
    "\n",
    "    def predict(self, X, feature_names):\n",
    "        clean_text = self._clean_text_transformer.transform(X)\n",
    "        spacy_tokens = self._spacy_tokenizer.transform(clean_text)\n",
    "        tfidf_features = self._tfidf_vectorizer.transform(spacy_tokens)\n",
    "        predictions = self._lr_model.predict_proba(tfidf_features)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is the study that the article is based on:\\r\\n\\r\\nhttps://www.nature.com/articles/nature25778.epdf']\n",
      "[[0.82791732 0.17208268]]\n"
     ]
    }
   ],
   "source": [
    "# test that our model works\n",
    "from RedditClassifier import RedditClassifier\n",
    "# With one sample\n",
    "sample = x_test[0:1]\n",
    "print(sample)\n",
    "print(RedditClassifier().predict(sample, [\"feature_name\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Docker Image with the S2i utility\n",
    "Using the S2I command line interface we wrap our current model to seve it through the Seldon interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM seldonio/seldon-core-s2i-python37-ubi8:1.7.0-dev\n",
    "\n",
    "RUN pip install spacy==2.3.2\n",
    "RUN python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 sha256:456ba4fb7975f8c66cb47f5783f3ff34ac4c535a63912c1d27ce72bbe0d63b83\n",
      "#1 transferring dockerfile: 173B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 sha256:8f400c1990a77c47cbbf40ed397dd023f5b85426e8239c6c07bef77255e0861a\n",
      "#2 transferring context: 2B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/seldonio/seldon-core-s2i-python37-ubi8:1.7.0-dev\n",
      "#3 sha256:0ea020e7a4239d8c0dfab255297ed4513a937f10c4eb4a0da2ede2aa401cf138\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [1/3] FROM docker.io/seldonio/seldon-core-s2i-python37-ubi8:1.7.0-dev\n",
      "#4 sha256:1c0985604a56cce763c19b93669933d75dbdf8b7fc62a5cc9aa635630df6aa28\n",
      "#4 CACHED\n",
      "\n",
      "#5 [2/3] RUN pip install spacy==2.3.2\n",
      "#5 sha256:d050cddbacda288e4faf815fd216ac3fee288c8e13762b13970f0493db32a1ba\n",
      "#5 0.863 Collecting spacy==2.3.2\n",
      "#5 0.967   Downloading spacy-2.3.2-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
      "#5 1.612 Collecting blis<0.5.0,>=0.4.0\n",
      "#5 1.640   Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
      "#5 2.153 Collecting thinc==7.4.1\n",
      "#5 2.182   Downloading thinc-7.4.1-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
      "#5 2.363 Collecting plac<1.2.0,>=0.9.6\n",
      "#5 2.385   Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "#5 2.480 Collecting murmurhash<1.1.0,>=0.28.0\n",
      "#5 2.496   Downloading murmurhash-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (20 kB)\n",
      "#5 2.601 Collecting preshed<3.1.0,>=3.0.2\n",
      "#5 2.618   Downloading preshed-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (126 kB)\n",
      "#5 2.694 Collecting catalogue<1.1.0,>=0.0.7\n",
      "#5 2.718   Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "#5 2.741 Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2) (1.20.1)\n",
      "#5 2.898 Collecting tqdm<5.0.0,>=4.38.0\n",
      "#5 2.914   Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "#5 2.941 Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2) (46.1.0)\n",
      "#5 2.963 Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2) (2.22.0)\n",
      "#5 3.034 Collecting cymem<2.1.0,>=2.0.2\n",
      "#5 3.059   Downloading cymem-2.0.5-cp37-cp37m-manylinux2014_x86_64.whl (35 kB)\n",
      "#5 3.136 Collecting wasabi<1.1.0,>=0.4.0\n",
      "#5 3.155   Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "#5 3.295 Collecting srsly<1.1.0,>=1.0.2\n",
      "#5 3.318   Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
      "#5 3.348 Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.2) (3.7.3)\n",
      "#5 3.370 Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (2020.12.5)\n",
      "#5 3.371 Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (3.0.4)\n",
      "#5 3.373 Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (2.8)\n",
      "#5 3.376 Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (1.25.9)\n",
      "#5 3.389 Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.2) (3.7.4.3)\n",
      "#5 3.392 Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.2) (3.4.1)\n",
      "#5 3.658 Installing collected packages: blis, tqdm, murmurhash, cymem, preshed, plac, catalogue, srsly, wasabi, thinc, spacy\n",
      "#5 3.751   Attempting uninstall: tqdm\n",
      "#5 3.757     Found existing installation: tqdm 4.36.1\n",
      "#5 3.763     Uninstalling tqdm-4.36.1:\n",
      "#5 3.776       Successfully uninstalled tqdm-4.36.1\n",
      "#5 5.165 Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.5 murmurhash-1.0.5 plac-1.1.3 preshed-3.0.5 spacy-2.3.2 srsly-1.0.5 thinc-7.4.1 tqdm-4.59.0 wasabi-0.8.2\n",
      "#5 5.190 WARNING: You are using pip version 20.2; however, version 21.0.1 is available.\n",
      "#5 5.190 You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
      "#5 DONE 5.4s\n",
      "\n",
      "#6 [3/3] RUN python -m spacy download en_core_web_sm\n",
      "#6 sha256:768291585909f254516f0462cd67c4d393536b55f38a15c94164394b5761e6a8\n",
      "#6 1.542 Collecting en_core_web_sm==2.3.1\n",
      "#6 1.867   Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "#6 2.868 Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "#6 2.904 Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\n",
      "#6 2.906 Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.20.1)\n",
      "#6 2.908 Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "#6 2.909 Requirement already satisfied: thinc==7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "#6 2.930 Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "#6 2.933 Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "#6 2.936 Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "#6 2.941 Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "#6 2.943 Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "#6 2.945 Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "#6 2.957 Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.59.0)\n",
      "#6 2.967 Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.22.0)\n",
      "#6 2.979 Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (46.1.0)\n",
      "#6 3.003 Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.3)\n",
      "#6 3.024 Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "#6 3.025 Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.8)\n",
      "#6 3.028 Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "#6 3.042 Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "#6 3.043 Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.1)\n",
      "#6 3.060 Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
      "#6 3.061 Building wheels for collected packages: en-core-web-sm\n",
      "#6 3.061   Building wheel for en-core-web-sm (setup.py): started\n",
      "#6 3.925   Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "#6 3.951   Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=b44642fe250e62c7dae48a8d4ccff708e700132c61d25b1d018256493578bab7\n",
      "#6 3.951   Stored in directory: /tmp/pip-ephem-wheel-cache-wmlcm06k/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\n",
      "#6 3.952 Successfully built en-core-web-sm\n",
      "#6 4.120 Installing collected packages: en-core-web-sm\n",
      "#6 4.249 Successfully installed en-core-web-sm-2.3.1\n",
      "#6 4.450 WARNING: You are using pip version 20.2; however, version 21.0.1 is available.\n",
      "#6 4.450 You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
      "#6 4.511 \u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "#6 4.511 You can now load the model via spacy.load('en_core_web_sm')\n",
      "#6 DONE 4.6s\n",
      "\n",
      "#7 exporting to image\n",
      "#7 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n",
      "#7 exporting layers\n",
      "#7 exporting layers 0.6s done\n",
      "#7 writing image sha256:8ca14f237d13fb2ad14dcb2621a69a246b9dca87bffcbb280af81485e4af9758 done\n",
      "#7 naming to docker.io/seldonio/seldon-core-spacy-base:0.1 done\n",
      "#7 DONE 0.6s\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build . -f Dockerfile -t seldonio/seldon-core-spacy-base:0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a docker image we need to create the s2i folder configuration as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME=RedditClassifier\r\n",
      "API_TYPE=REST\r\n",
      "SERVICE_TYPE=MODEL\r\n",
      "PERSISTENCE=0\r\n"
     ]
    }
   ],
   "source": [
    "!cat .s2i/environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn>=0.23.2\r\n",
      "spacy==2.3.2\r\n",
      "dill==0.3.2\r\n",
      "pandas==1.1.1\r\n"
     ]
    }
   ],
   "source": [
    "# As well as a requirements.txt file with all the relevant dependencies\n",
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "---> Installing application source...\n",
      "---> Installing dependencies ...\n",
      "Looking in links: /whl\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Collecting scikit-learn>=0.23.2\n",
      "Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "Requirement already satisfied: spacy==2.3.2 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (2.3.2)\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Collecting dill==0.3.2\n",
      "Downloading dill-0.3.2.zip (177 kB)\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Collecting pandas==1.1.1\n",
      "Downloading pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Collecting scipy>=0.19.1\n",
      "Downloading scipy-1.6.1-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.23.2->-r requirements.txt (line 1)) (1.20.1)\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Collecting joblib>=0.11\n",
      "Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (3.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (2.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (4.59.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (7.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (2.22.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (46.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (0.8.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy==2.3.2->-r requirements.txt (line 2)) (1.0.0)\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Collecting python-dateutil>=2.7.3\n",
      "Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "Collecting pytz>=2017.2\n",
      "Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2->-r requirements.txt (line 2)) (2020.12.5)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.2->-r requirements.txt (line 2)) (3.7.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.1.1->-r requirements.txt (line 4)) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.2->-r requirements.txt (line 2)) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.2->-r requirements.txt (line 2)) (3.4.1)\n",
      "Building wheels for collected packages: dill\n",
      "Building wheel for dill (setup.py): started\n",
      "Building wheel for dill (setup.py): finished with status 'done'\n",
      "Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78912 sha256=484ab95ec424c23e0b5d0e92279e0dd99d7b5c337ab85d7a926596cc0028b947\n",
      "Stored in directory: /tmp/pip-ephem-wheel-cache-3emqefhy/wheels/72/6b/d5/5548aa1b73b8c3d176ea13f9f92066b02e82141549d90e2100\n",
      "Successfully built dill\n",
      "Installing collected packages: scipy, threadpoolctl, joblib, scikit-learn, dill, python-dateutil, pytz, pandas\n",
      "Successfully installed dill-0.3.2 joblib-1.0.1 pandas-1.1.1 python-dateutil-2.8.1 pytz-2021.1 scikit-learn-0.24.1 scipy-1.6.1 threadpoolctl-2.1.0\n",
      "WARNING: Url '/whl' is ignored. It is either a non-existing path or lacks a specific scheme.\n",
      "WARNING: You are using pip version 20.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
      "Collecting pip-licenses\n",
      "Downloading pip_licenses-3.3.1-py3-none-any.whl (16 kB)\n",
      "Collecting PTable\n",
      "Downloading PTable-0.9.2.tar.gz (31 kB)\n",
      "Building wheels for collected packages: PTable\n",
      "Building wheel for PTable (setup.py): started\n",
      "Building wheel for PTable (setup.py): finished with status 'done'\n",
      "Created wheel for PTable: filename=PTable-0.9.2-py3-none-any.whl size=22907 sha256=8475610d2b71640b1d3866997bd82d7de2ccb5ed12de195ddc11a8f1dc05a75f\n",
      "Stored in directory: /root/.cache/pip/wheels/33/df/2f/674985b3f8a2de3f96357d1eadef5110f74fa91b3785e52a54\n",
      "Successfully built PTable\n",
      "Installing collected packages: PTable, pip-licenses\n",
      "Successfully installed PTable-0.9.2 pip-licenses-3.3.1\n",
      "WARNING: You are using pip version 20.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
      "created path: ./licenses/license_info.csv\n",
      "created path: ./licenses/license.txt\n",
      "Build completed successfully\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "s2i build . seldonio/seldon-core-spacy-base:0.1 seldonio/reddit-classifier:0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Test your model as a docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No such container: reddit_predictor\r\n"
     ]
    }
   ],
   "source": [
    "# Remove previously deployed containers for this model\n",
    "!docker rm -f reddit_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d594705a8a21e8d8994af0b24e28b3c1510e9050fa5c24146eeaac55f496f73e\r\n"
     ]
    }
   ],
   "source": [
    "!docker run --name \"reddit_predictor\" -d --rm -p 9001:9000 seldonio/reddit-classifier:0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Unnecessary use of -X or --request, POST is already inferred.\r\n",
      "*   Trying 127.0.0.1:9001...\r\n",
      "* TCP_NODELAY set\r\n",
      "* Connected to localhost (127.0.0.1) port 9001 (#0)\r\n",
      "> POST /api/v1.0/predictions HTTP/1.1\r",
      "\r\n",
      "> Host: localhost:9001\r",
      "\r\n",
      "> User-Agent: curl/7.68.0\r",
      "\r\n",
      "> Accept: */*\r",
      "\r\n",
      "> Content-Type: application/json\r",
      "\r\n",
      "> Content-Length: 76\r",
      "\r\n",
      "> \r",
      "\r\n",
      "* upload completely sent off: 76 out of 76 bytes\r\n",
      "* Mark bundle as not supporting multiuse\r\n",
      "< HTTP/1.1 200 OK\r",
      "\r\n",
      "< Server: gunicorn/20.0.4\r",
      "\r\n",
      "< Date: Thu, 18 Mar 2021 14:23:16 GMT\r",
      "\r\n",
      "< Connection: keep-alive\r",
      "\r\n",
      "< Content-Type: application/json\r",
      "\r\n",
      "< Content-Length: 95\r",
      "\r\n",
      "< Access-Control-Allow-Origin: *\r",
      "\r\n",
      "< \r",
      "\r\n",
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.535398662890584,0.46460133710941603]]},\"meta\":{}}\r\n",
      "* Connection #0 to host localhost left intact\r\n"
     ]
    }
   ],
   "source": [
    "!curl -v -X POST -H 'Content-Type: application/json' \\\n",
    "   -d '{\"data\": { \"ndarray\": [\"this is a terrible comment\"], \"names\": [\"tfidf\"] } }' \\\n",
    "    http://localhost:9001/api/v1.0/predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:True message:\n",
      "Request:\n",
      "data {\n",
      "  names: \"tfidf\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      string_value: \"This is the study that the article is based on:\\r\\n\\r\\nhttps://www.nature.com/articles/nature25778.epdf\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Response:\n",
      "meta {\n",
      "}\n",
      "data {\n",
      "  names: \"t:0\"\n",
      "  names: \"t:1\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      list_value {\n",
      "        values {\n",
      "          number_value: 0.8279173241631921\n",
      "        }\n",
      "        values {\n",
      "          number_value: 0.1720826758368079\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We now test the REST endpoint expecting the same result\n",
    "endpoint = \"0.0.0.0:9001\"\n",
    "batch = sample\n",
    "payload_type = \"ndarray\"\n",
    "\n",
    "sc = SeldonClient(microservice_endpoint=endpoint)\n",
    "response = sc.microservice(\n",
    "    data=batch,\n",
    "    method=\"predict\",\n",
    "    payload_type=payload_type,\n",
    "    names=[\"tfidf\"])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_predictor\r\n"
     ]
    }
   ],
   "source": [
    "# We now stop it to run it in docker\n",
    "!docker stop reddit_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Run Seldon in your kubernetes cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Seldon Core\n",
    "\n",
    "Use the setup notebook to [Setup Cluster](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html#Setup-Cluster) with [Ambassador Ingress](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html#Ambassador) or Istio and [Install Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html#Install-Seldon-Core). Instructions [also online](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Deploy your model with Seldon\n",
    "We can now deploy our model by using the Seldon graph definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reddit_clf.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile reddit_clf.json\n",
    "{\n",
    "    \"apiVersion\": \"machinelearning.seldon.io/v1alpha2\",\n",
    "    \"kind\": \"SeldonDeployment\",\n",
    "    \"metadata\": {\n",
    "        \"labels\": {\n",
    "            \"app\": \"seldon\"\n",
    "        },\n",
    "        \"name\": \"reddit-classifier\"\n",
    "    },\n",
    "    \"spec\": {\n",
    "        \"annotations\": {\n",
    "            \"project_name\": \"Reddit classifier\",\n",
    "            \"deployment_version\": \"v1\"\n",
    "        },\n",
    "        \"name\": \"reddit-classifier\",\n",
    "        \"predictors\": [\n",
    "            {\n",
    "                \"componentSpecs\": [{\n",
    "                    \"spec\": {\n",
    "                        \"containers\": [\n",
    "                            {\n",
    "                                \"image\": \"seldonio/reddit-classifier:0.1\",\n",
    "                                \"imagePullPolicy\": \"IfNotPresent\",\n",
    "                                \"name\": \"classifier\",\n",
    "                                \"resources\": {\n",
    "                                    \"requests\": {\n",
    "                                        \"memory\": \"1Mi\"\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        ],\n",
    "                        \"terminationGracePeriodSeconds\": 20\n",
    "                    }\n",
    "                }],\n",
    "                \"graph\": {\n",
    "                    \"children\": [],\n",
    "                    \"name\": \"classifier\",\n",
    "                    \"endpoint\": {\n",
    "            \"type\" : \"REST\"\n",
    "            },\n",
    "                    \"type\": \"MODEL\"\n",
    "                },\n",
    "                \"name\": \"single-model\",\n",
    "                \"replicas\": 1,\n",
    "        \"annotations\": {\n",
    "            \"predictor_version\" : \"v1\"\n",
    "        }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you are using kind preload image first with\n",
    "```bash\n",
    "kind load docker-image reddit-classifier:0.1 --name <name of your cluster>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io/reddit-classifier unchanged\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f reddit_clf.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                           READY   STATUS    RESTARTS   AGE\r\n",
      "reddit-classifier-single-model-0-classifier-78d5cf779d-btxqg   2/2     Running   0          27s\r\n",
      "seldon-92a927e5e90d7602e08ba9b9304f70e8-5bcf96696f-6pwbt       1/2     Running   2          2d23h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Interact with your model through API\n",
    "Now that our Seldon Deployment is live, we are able to interact with it through its API.\n",
    "\n",
    "There are two options in which we can interact with our new model. These are:\n",
    "\n",
    "a) Using CURL from the CLI (or another rest client like Postman)\n",
    "\n",
    "b) Using the Python SeldonClient\n",
    "\n",
    "#### a) Using CURL from the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"ndarray\":[[0.6811752552555037,0.3188247447444963]]},\"meta\":{\"requestPath\":{\"classifier\":\"seldonio/reddit-classifier:0.1\"}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 127.0.0.1:80...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 80 (#0)\n",
      "> POST /seldon/default/reddit-classifier/api/v1.0/predictions HTTP/1.1\r\n",
      "> Host: localhost\r\n",
      "> User-Agent: curl/7.68.0\r\n",
      "> Accept: */*\r\n",
      "> Content-Type: application/json\r\n",
      "> Content-Length: 72\r\n",
      "> \r\n",
      "} [72 bytes data]\n",
      "* upload completely sent off: 72 out of 72 bytes\n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\r\n",
      "< access-control-allow-headers: Accept, Accept-Encoding, Authorization, Content-Length, Content-Type, X-CSRF-Token\r\n",
      "< access-control-allow-methods: OPTIONS,POST\r\n",
      "< access-control-allow-origin: *\r\n",
      "< content-type: application/json\r\n",
      "< seldon-puid: 7b586b15-2a7a-4d36-aa39-210b24cb5841\r\n",
      "< x-content-type-options: nosniff\r\n",
      "< date: Fri, 19 Mar 2021 09:47:09 GMT\r\n",
      "< content-length: 156\r\n",
      "< x-envoy-upstream-service-time: 19\r\n",
      "< server: istio-envoy\r\n",
      "< \r\n",
      "{ [156 bytes data]\n",
      "\r",
      "100   228  100   156  100    72   4000   1846 --:--:-- --:--:-- --:--:--  5846\n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -v -H 'Content-Type: application/json' \\\n",
    "    -d '{\"data\": {\"names\": [\"text\"], \"ndarray\": [\"Hello world this is a test\"]}}' \\\n",
    "    http://localhost:80/seldon/default/reddit-classifier/api/v1.0/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Using the Python SeldonClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success:True message:\n",
      "Request:\n",
      "meta {\n",
      "}\n",
      "data {\n",
      "  names: \"text\"\n",
      "  ndarray {\n",
      "    values {\n",
      "      string_value: \"Hello world this is a test\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Response:\n",
      "{'data': {'names': ['t:0', 't:1'], 'ndarray': [[0.6811752552555037, 0.3188247447444963]]}, 'meta': {'requestPath': {'classifier': 'seldonio/reddit-classifier:0.1'}}}\n"
     ]
    }
   ],
   "source": [
    "from seldon_core.seldon_client import SeldonClient\n",
    "import numpy as np\n",
    "\n",
    "sc = SeldonClient(\n",
    "    gateway=\"ambassador\", \n",
    "    transport=\"rest\",\n",
    "    gateway_endpoint=\"localhost:80\",   # Make sure you use the port above\n",
    "    namespace=\"default\"\n",
    ")\n",
    "\n",
    "client_prediction = sc.predict(\n",
    "    data=np.array([\"Hello world this is a test\"]), \n",
    "    deployment_name=\"reddit-classifier\",\n",
    "    names=[\"text\"],\n",
    "    payload_type=\"ndarray\",\n",
    ")\n",
    "\n",
    "print(client_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Clean your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io \"reddit-classifier\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f reddit_clf.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
