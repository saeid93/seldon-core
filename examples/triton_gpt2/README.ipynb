{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-toronto",
   "metadata": {},
   "source": [
    "# Pretrained  GPT2  Model Deployment Example\n",
    "\n",
    "In this notebook, we will run an example of text generation using GPT2 model exported from HuggingFace and deployed with Seldon's Triton pre-packed server. the example also covers converting the model to ONNX format.\n",
    "The implemented example below is of the Greedy approach for the next token prediction.\n",
    "more info: https://huggingface.co/transformers/model_doc/gpt2.html?highlight=gpt2\n",
    "\n",
    "After we have the module deployed to Kubernetes, we will run a simple load test to evaluate the module inference performance.\n",
    "\n",
    "\n",
    "## Steps:\n",
    "1. Download pretrained GPT2 model from hugging face\n",
    "2. Convert the model to ONNX\n",
    "3. Store it in MinIo bucket\n",
    "4. Setup Seldon-Core in your kubernetes cluster\n",
    "5. Deploy the ONNX model with Seldonâ€™s prepackaged Triton server.\n",
    "6. Interact with the model, run a greedy alg example (generate sentence completion)\n",
    "7. Run load test using vegeta\n",
    "8. Clean-up\n",
    "\n",
    "## Basic requirements\n",
    "* Helm v3.0.0+\n",
    "* A Kubernetes cluster running v1.13 or above (minkube / docker-for-windows work well if enough RAM)\n",
    "* kubectl v1.14+\n",
    "* Python 3.6+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "korean-reporter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "transformers==4.5.1\n",
    "torch==1.8.1\n",
    "tokenizers<0.11,>=0.10.1\n",
    "tensorflow==2.4.1\n",
    "tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assigned-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.5.1 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (4.5.1)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.8.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (0.10.3)\n",
      "Requirement already satisfied: tensorflow==2.4.1 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: tf2onnx in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from transformers==4.5.1->-r requirements.txt (line 1)) (2022.6.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from transformers==4.5.1->-r requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: sacremoses in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from transformers==4.5.1->-r requirements.txt (line 1)) (0.0.53)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from transformers==4.5.1->-r requirements.txt (line 1)) (4.64.0)\n",
      "Requirement already satisfied: filelock in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from transformers==4.5.1->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: requests in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from transformers==4.5.1->-r requirements.txt (line 1)) (2.28.0)\n",
      "Requirement already satisfied: packaging in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from transformers==4.5.1->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from torch==1.8.1->-r requirements.txt (line 2)) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (1.12)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (1.15.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (2.10.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (1.6.3)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (0.37.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (0.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (2.4.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (1.32.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (2.9.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (3.19.4)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (0.15.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorflow==2.4.1->-r requirements.txt (line 4)) (1.12.1)\n",
      "Requirement already satisfied: onnx>=1.4.1 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tf2onnx->-r requirements.txt (line 5)) (1.12.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (61.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (3.3.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (1.35.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 1)) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 1)) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from requests->transformers==4.5.1->-r requirements.txt (line 1)) (2022.6.15)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1->-r requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from packaging->transformers==4.5.1->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: click in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 1)) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/cc/miniconda3/envs/central/lib/python3.8/site-packages (from sacremoses->transformers==4.5.1->-r requirements.txt (line 1)) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-evaluation",
   "metadata": {},
   "source": [
    "### Export HuggingFace TFGPT2LMHeadModel pre-trained model and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "import transformers\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\", from_pt=True, pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "model.save_pretrained(\"./tfgpt2model\", saved_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-tribute",
   "metadata": {},
   "source": [
    "### Convert the TensorFlow saved model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "irish-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-29 16:49:08.077603: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-29 16:49:08.077636: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/cc/miniconda3/envs/central/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-07-29 16:49:09.328432: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 16:49:09.328624: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-29 16:49:09.328636: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-29 16:49:09.328654: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (k8s-cluster): /proc/driver/nvidia/version does not exist\n",
      "2022-07-29 16:49:09.328881: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-29 16:49:09.334414: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 16:49:09,343 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-07-29 16:49:14,382 - INFO - Signatures found in model: [serving_default].\n",
      "2022-07-29 16:49:14,382 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-07-29 16:49:14,383 - INFO - Output names: ['logits', 'past_key_values']\n",
      "2022-07-29 16:49:14.433375: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-07-29 16:49:14.433503: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2022-07-29 16:49:14.433789: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 16:49:14.461154: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz\n",
      "2022-07-29 16:49:14.604296: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 3213 nodes (3060), 4128 edges (3974), time = 75.745ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 1.633ms.\n",
      "\n",
      "2022-07-29 16:49:28.515956: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "WARNING:tensorflow:From /home/cc/miniconda3/envs/central/lib/python3.8/site-packages/tf2onnx/tf_loader.py:711: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-07-29 16:49:31,097 - WARNING - From /home/cc/miniconda3/envs/central/lib/python3.8/site-packages/tf2onnx/tf_loader.py:711: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-07-29 16:49:31.168765: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-07-29 16:49:31.168935: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2022-07-29 16:49:31.169462: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 16:49:39.820789: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 2720 nodes (-318), 3646 edges (-319), time = 5761.87598ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 3.25ms.\n",
      "  constant_folding: Graph size after: 2720 nodes (0), 3646 edges (0), time = 1354.18396ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 10.528ms.\n",
      "\n",
      "2022-07-29 16:49:44,342 - INFO - Using tensorflow=2.4.1, onnx=1.12.0, tf2onnx=1.11.1/1915fb\n",
      "2022-07-29 16:49:44,343 - INFO - Using opset <onnx, 11>\n",
      "2022-07-29 16:50:15,651 - INFO - Computed 0 values for constant folding\n",
      "2022-07-29 16:50:34,885 - INFO - Optimizing ONNX model\n",
      "2022-07-29 16:50:43,042 - INFO - After optimization: Cast -147 (311->164), Concat -49 (126->77), Const -1854 (2032->178), GlobalAveragePool +50 (0->50), Identity -76 (76->0), ReduceMean -50 (50->0), Shape -49 (112->63), Slice -98 (235->137), Squeeze -198 (223->25), Transpose -1 (61->60), Unsqueeze -361 (435->74)\n",
      "2022-07-29 16:50:45,325 - INFO - \n",
      "2022-07-29 16:50:45,325 - INFO - Successfully converted TensorFlow model ./tfgpt2model/saved_model/1 to ONNX\n",
      "2022-07-29 16:50:45,325 - INFO - Model inputs: ['attention_mask', 'input_ids']\n",
      "2022-07-29 16:50:45,325 - INFO - Model outputs: ['logits', 'past_key_values']\n",
      "2022-07-29 16:50:45,325 - INFO - ONNX model is saved at model.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --saved-model ./tfgpt2model/saved_model/1 --opset 11  --output model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-pantyhose",
   "metadata": {},
   "source": [
    "### Copy your model to a local MinIo\n",
    "#### Setup MinIo\n",
    "Use the provided [notebook](https://docs.seldon.io/projects/seldon-core/en/latest/examples/minio_setup.html) to install MinIo in your cluster and configure `mc` CLI tool. Instructions also [online](https://docs.min.io/docs/minio-client-quickstart-guide.html).\n",
    "\n",
    "-- Note: You can use your prefer remote storage server (google/ AWS etc.)\n",
    "\n",
    "#### Create a Bucket and store your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lasting-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32;1mBucket created successfully `minio/language-models/onnx-gpt2/1`.\u001b[0m\n",
      "...odel.onnx:  622.29 MiB / 622.29 MiB â”ƒâ–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â”ƒ 133.56 MiB/s 4s\u001b[0m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "!mc mb minio/language-models/onnx-gpt2/1 -p\n",
    "!mc cp ./model.onnx minio/language-models/onnx-gpt2/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-syracuse",
   "metadata": {},
   "source": [
    "### Run Seldon in your kubernetes cluster\n",
    "\n",
    "Follow the [Seldon-Core Setup notebook](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html) to Setup a cluster with Ambassador Ingress or Istio and install Seldon Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-outreach",
   "metadata": {},
   "source": [
    "### Deploy your model with Seldon pre-packaged Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "declared-crown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing secret.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile secret.yaml\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: seldon-init-container-secret\n",
    "type: Opaque\n",
    "stringData:\n",
    "  RCLONE_CONFIG_S3_TYPE: s3\n",
    "  RCLONE_CONFIG_S3_PROVIDER: minio\n",
    "  RCLONE_CONFIG_S3_ENV_AUTH: \"false\"\n",
    "  RCLONE_CONFIG_S3_ACCESS_KEY_ID: minioadmin\n",
    "  RCLONE_CONFIG_S3_SECRET_ACCESS_KEY: minioadmin\n",
    "  RCLONE_CONFIG_S3_ENDPOINT: http://minio.minio-system.svc.cluster.local:9000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beneficial-anime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gpt2-deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile gpt2-deploy.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: gpt2\n",
    "spec:\n",
    "  predictors:\n",
    "  - graph:\n",
    "      implementation: TRITON_SERVER\n",
    "      logger:\n",
    "        mode: all\n",
    "      modelUri: s3://language-models\n",
    "      envSecretRefName: seldon-init-container-secret\n",
    "      name: onnx-gpt2\n",
    "      type: MODEL\n",
    "    name: default\n",
    "    replicas: 1\n",
    "  protocol: kfserving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "subjective-involvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/seldon-init-container-secret configured\n",
      "seldondeployment.machinelearning.seldon.io/gpt2 unchanged\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f secret.yaml -n default\n",
    "!kubectl apply -f gpt2-deploy.yaml -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "demanding-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment \"gpt2-default-0-onnx-gpt2\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "deployment \"gpt2-default-0-onnx-gpt2\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=gpt2 -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-supervisor",
   "metadata": {},
   "source": [
    "#### Interact with the model: get model metadata (a \"test\" request to make sure our model is available and loaded correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "married-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"onnx-gpt2\",\"versions\":[\"1\"],\"platform\":\"onnxruntime_onnx\",\"inputs\":[{\"name\":\"input_ids\",\"datatype\":\"INT32\",\"shape\":[-1,-1]},{\"name\":\"attention_mask\",\"datatype\":\"INT32\",\"shape\":[-1,-1]}],\"outputs\":[{\"name\":\"past_key_values\",\"datatype\":\"FP32\",\"shape\":[12,2,-1,12,-1,64]},{\"name\":\"logits\",\"datatype\":\"FP32\",\"shape\":[-1,-1,50257]}]}"
     ]
    }
   ],
   "source": [
    "!curl -s http://localhost:32000/seldon/default/gpt2/v2/models/onnx-gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-resource",
   "metadata": {},
   "source": [
    "### Run prediction test: generate a sentence completion using GPT2 model  - Greedy approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "modified-termination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: I enjoy working in Seldon\n",
      "Output: I enjoy working in Seldon 's office , and I 'm glad to see that\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_text = \"I enjoy working in Seldon\"\n",
    "count = 0\n",
    "max_gen_len = 10\n",
    "gen_sentence = input_text\n",
    "while count < max_gen_len:\n",
    "    input_ids = tokenizer.encode(gen_sentence, return_tensors=\"tf\")\n",
    "    shape = input_ids.shape.as_list()\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"input_ids\",\n",
    "                \"datatype\": \"INT32\",\n",
    "                \"shape\": shape,\n",
    "                \"data\": input_ids.numpy().tolist(),\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"attention_mask\",\n",
    "                \"datatype\": \"INT32\",\n",
    "                \"shape\": shape,\n",
    "                \"data\": np.ones(shape, dtype=np.int32).tolist(),\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    ret = requests.post(\n",
    "        \"http://localhost:32000/seldon/default/gpt2/v2/models/onnx-gpt2/infer\",\n",
    "        json=payload,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        res = ret.json()\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # extract logits\n",
    "    logits = np.array(res[\"outputs\"][1][\"data\"])\n",
    "    logits = logits.reshape(res[\"outputs\"][1][\"shape\"])\n",
    "\n",
    "    # take the best next token probability of the last token of input ( greedy approach)\n",
    "    next_token = logits.argmax(axis=2)[0]\n",
    "    next_token_str = tokenizer.decode(\n",
    "        next_token[-1:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    ).strip()\n",
    "    gen_sentence += \" \" + next_token_str\n",
    "    count += 1\n",
    "\n",
    "print(f\"Input: {input_text}\\nOutput: {gen_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-status",
   "metadata": {},
   "source": [
    "### Run Load Test / Performance Test using vegeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-discovery",
   "metadata": {},
   "source": [
    "#### Install vegeta, for more details take a look in [vegeta](https://github.com/tsenart/vegeta#install) official documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/tsenart/vegeta/releases/download/v12.8.3/vegeta-12.8.3-linux-amd64.tar.gz\n",
    "!tar -zxvf vegeta-12.8.3-linux-amd64.tar.gz\n",
    "!chmod +x vegeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-lying",
   "metadata": {},
   "source": [
    "#### Generate vegeta [target file](https://github.com/tsenart/vegeta#-targets) contains \"post\" cmd with payload in the requiered structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "reliable-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from subprocess import PIPE, Popen, run\n",
    "\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_text = \"I enjoy working in Seldon\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "shape = input_ids.shape.as_list()\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"input_ids\",\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"shape\": shape,\n",
    "            \"data\": input_ids.numpy().tolist(),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"attention_mask\",\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"shape\": shape,\n",
    "            \"data\": np.ones(shape, dtype=np.int32).tolist(),\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "cmd = {\n",
    "    \"method\": \"POST\",\n",
    "    \"header\": {\"Content-Type\": [\"application/json\"]},\n",
    "    \"url\": \"http://localhost:80/seldon/default/gpt2/v2/models/gpt2/infer\",\n",
    "    \"body\": base64.b64encode(bytes(json.dumps(payload), \"utf-8\")).decode(\"utf-8\"),\n",
    "}\n",
    "\n",
    "with open(\"vegeta_target.json\", mode=\"w\") as file:\n",
    "    json.dump(cmd, file)\n",
    "    file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vegeta attack -targets=vegeta_target.json -rate=1 -duration=60s -format=json | vegeta report -type=text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-suite",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pacific-collectible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io \"gpt2\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f gpt2-deploy.yaml -n default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('central')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2465c4f56298bc06dbdad3e7519856d346ec0e9edf6ba2c905f0af711583810e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
