{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-toronto",
   "metadata": {},
   "source": [
    "# Pretrained  GPT2  Model Deployment Example\n",
    "\n",
    "In this notebook, we will run an example of text generation using GPT2 model exported from HuggingFace and deployed with Seldon's Triton pre-packed server. the example also covers converting the model to ONNX format.\n",
    "The implemented example below is of the Greedy approach for the next token prediction.\n",
    "more info: https://huggingface.co/transformers/model_doc/gpt2.html?highlight=gpt2\n",
    "\n",
    "After we have the module deployed to Kubernetes, we will run a simple load test to evaluate the module inference performance.\n",
    "\n",
    "\n",
    "## Steps:\n",
    "1. Download pretrained GPT2 model from hugging face\n",
    "2. Convert the model to ONNX\n",
    "3. Store it in MinIo bucket\n",
    "4. Setup Seldon-Core in your kubernetes cluster\n",
    "5. Deploy the ONNX model with Seldonâ€™s prepackaged Triton server.\n",
    "6. Interact with the model, run a greedy alg example (generate sentence completion)\n",
    "7. Run load test using vegeta\n",
    "8. Clean-up\n",
    "\n",
    "## Basic requirements\n",
    "* Helm v3.0.0+\n",
    "* A Kubernetes cluster running v1.13 or above (minkube / docker-for-windows work well if enough RAM)\n",
    "* kubectl v1.14+\n",
    "* Python 3.6+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "korean-reporter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "transformers==4.5.1\n",
    "torch==1.8.1\n",
    "tokenizers<0.11,>=0.10.1\n",
    "tensorflow==2.4.1\n",
    "tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-evaluation",
   "metadata": {},
   "source": [
    "### Export HuggingFace TFGPT2LMHeadModel pre-trained model and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "iraqi-million",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 17:05:48.930614: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-29 17:05:48.930638: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-29 17:05:51.023887: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 17:05:51.024040: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-29 17:05:51.024048: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-29 17:05:51.024061: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (k8s-cluster): /proc/driver/nvidia/version does not exist\n",
      "2022-07-29 17:05:51.024241: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-29 17:05:51.029822: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 17:05:51.051105: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fb853d9fb20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7fb853d9fb20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as wte_layer_call_and_return_conditional_losses, wte_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_layer_call_fn, ln_f_layer_call_and_return_conditional_losses while saving (showing 5 of 735). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as wte_layer_call_and_return_conditional_losses, wte_layer_call_fn, dropout_layer_call_and_return_conditional_losses, dropout_layer_call_fn, ln_f_layer_call_and_return_conditional_losses while saving (showing 5 of 735). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tfgpt2model1/saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tfgpt2model1/saved_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "import transformers\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\", from_pt=True, pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "model.save_pretrained(\"./tfgpt2model1\", saved_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a6cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:absl:Found untraced functions such as wte_layer_call_and_return_conditional_losses, wte_layer_call_fn, dropout_37_layer_call_and_return_conditional_losses, dropout_37_layer_call_fn, ln_f_layer_call_and_return_conditional_losses while saving (showing 5 of 735). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as wte_layer_call_and_return_conditional_losses, wte_layer_call_fn, dropout_37_layer_call_and_return_conditional_losses, dropout_37_layer_call_fn, ln_f_layer_call_and_return_conditional_losses while saving (showing 5 of 735). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tfgpt2model2/saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tfgpt2model2/saved_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\", from_pt=True, pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "model.save_pretrained(\"./tfgpt2model2\", saved_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-tribute",
   "metadata": {},
   "source": [
    "### Convert the TensorFlow saved model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "irish-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-29 17:13:27.562189: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-29 17:13:27.562216: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/cc/miniconda3/envs/central/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-07-29 17:13:28.839030: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 17:13:28.839181: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-29 17:13:28.839193: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-29 17:13:28.839210: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (k8s-cluster): /proc/driver/nvidia/version does not exist\n",
      "2022-07-29 17:13:28.839415: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-29 17:13:28.843949: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 17:13:28,849 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-07-29 17:13:33,982 - INFO - Signatures found in model: [serving_default].\n",
      "2022-07-29 17:13:33,982 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-07-29 17:13:33,983 - INFO - Output names: ['logits', 'past_key_values']\n",
      "2022-07-29 17:13:34.029722: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-07-29 17:13:34.029887: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2022-07-29 17:13:34.030183: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 17:13:34.061189: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz\n",
      "2022-07-29 17:13:34.192177: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 3213 nodes (3060), 4128 edges (3974), time = 70.853ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 1.457ms.\n",
      "\n",
      "2022-07-29 17:13:43.917258: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "WARNING:tensorflow:From /home/cc/miniconda3/envs/central/lib/python3.8/site-packages/tf2onnx/tf_loader.py:711: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-07-29 17:13:45,787 - WARNING - From /home/cc/miniconda3/envs/central/lib/python3.8/site-packages/tf2onnx/tf_loader.py:711: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-07-29 17:13:45.857657: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-07-29 17:13:45.857820: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2022-07-29 17:13:45.858156: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 17:13:56.150117: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 2720 nodes (-318), 3646 edges (-319), time = 7177.14111ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 3.146ms.\n",
      "  constant_folding: Graph size after: 2720 nodes (0), 3646 edges (0), time = 2048.76904ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 17.292ms.\n",
      "\n",
      "2022-07-29 17:14:00,412 - INFO - Using tensorflow=2.4.1, onnx=1.12.0, tf2onnx=1.11.1/1915fb\n",
      "2022-07-29 17:14:00,413 - INFO - Using opset <onnx, 11>\n",
      "2022-07-29 17:14:31,618 - INFO - Computed 0 values for constant folding\n",
      "2022-07-29 17:14:51,511 - INFO - Optimizing ONNX model\n",
      "2022-07-29 17:14:59,841 - INFO - After optimization: Cast -147 (311->164), Concat -49 (126->77), Const -1854 (2032->178), GlobalAveragePool +50 (0->50), Identity -76 (76->0), ReduceMean -50 (50->0), Shape -49 (112->63), Slice -98 (235->137), Squeeze -198 (223->25), Transpose -1 (61->60), Unsqueeze -361 (435->74)\n",
      "2022-07-29 17:15:00,685 - INFO - \n",
      "2022-07-29 17:15:00,685 - INFO - Successfully converted TensorFlow model ./tfgpt2model1/saved_model/1 to ONNX\n",
      "2022-07-29 17:15:00,685 - INFO - Model inputs: ['attention_mask', 'input_ids']\n",
      "2022-07-29 17:15:00,685 - INFO - Model outputs: ['logits', 'past_key_values']\n",
      "2022-07-29 17:15:00,685 - INFO - ONNX model is saved at model1/model.onnx\n",
      "2022-07-29 17:15:01.799961: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-29 17:15:01.799988: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/cc/miniconda3/envs/central/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-07-29 17:15:03.019120: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 17:15:03.019233: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-29 17:15:03.019243: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-29 17:15:03.019259: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (k8s-cluster): /proc/driver/nvidia/version does not exist\n",
      "2022-07-29 17:15:03.019448: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-29 17:15:03.023836: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 17:15:03,028 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "2022-07-29 17:15:08,090 - INFO - Signatures found in model: [serving_default].\n",
      "2022-07-29 17:15:08,090 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
      "2022-07-29 17:15:08,091 - INFO - Output names: ['logits', 'past_key_values']\n",
      "2022-07-29 17:15:08.137186: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-07-29 17:15:08.137314: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2022-07-29 17:15:08.137639: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 17:15:08.169098: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz\n",
      "2022-07-29 17:15:08.302996: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 3213 nodes (3060), 4128 edges (3974), time = 73.529ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 1.474ms.\n",
      "\n",
      "2022-07-29 17:15:18.076410: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "WARNING:tensorflow:From /home/cc/miniconda3/envs/central/lib/python3.8/site-packages/tf2onnx/tf_loader.py:711: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-07-29 17:15:20,141 - WARNING - From /home/cc/miniconda3/envs/central/lib/python3.8/site-packages/tf2onnx/tf_loader.py:711: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "2022-07-29 17:15:20.210779: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2022-07-29 17:15:20.210941: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2022-07-29 17:15:20.211297: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-29 17:15:30.527513: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  constant_folding: Graph size after: 2720 nodes (-318), 3646 edges (-319), time = 7144.23193ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 3.489ms.\n",
      "  constant_folding: Graph size after: 2720 nodes (0), 3646 edges (0), time = 2020.70398ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 11.981ms.\n",
      "\n",
      "2022-07-29 17:15:34,482 - INFO - Using tensorflow=2.4.1, onnx=1.12.0, tf2onnx=1.11.1/1915fb\n",
      "2022-07-29 17:15:34,482 - INFO - Using opset <onnx, 11>\n",
      "2022-07-29 17:16:05,157 - INFO - Computed 0 values for constant folding\n",
      "2022-07-29 17:16:24,688 - INFO - Optimizing ONNX model\n",
      "2022-07-29 17:16:33,165 - INFO - After optimization: Cast -147 (311->164), Concat -49 (126->77), Const -1854 (2032->178), GlobalAveragePool +50 (0->50), Identity -76 (76->0), ReduceMean -50 (50->0), Shape -49 (112->63), Slice -98 (235->137), Squeeze -198 (223->25), Transpose -1 (61->60), Unsqueeze -361 (435->74)\n",
      "2022-07-29 17:16:33,824 - INFO - \n",
      "2022-07-29 17:16:33,825 - INFO - Successfully converted TensorFlow model ./tfgpt2model2/saved_model/1 to ONNX\n",
      "2022-07-29 17:16:33,825 - INFO - Model inputs: ['attention_mask', 'input_ids']\n",
      "2022-07-29 17:16:33,825 - INFO - Model outputs: ['logits', 'past_key_values']\n",
      "2022-07-29 17:16:33,825 - INFO - ONNX model is saved at model2/model.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --saved-model ./tfgpt2model1/saved_model/1 --opset 11  --output model1/model.onnx\n",
    "!python -m tf2onnx.convert --saved-model ./tfgpt2model2/saved_model/1 --opset 11  --output model2/model.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-pantyhose",
   "metadata": {},
   "source": [
    "### Copy your model to a local MinIo\n",
    "#### Setup MinIo\n",
    "Use the provided [notebook](https://docs.seldon.io/projects/seldon-core/en/latest/examples/minio_setup.html) to install MinIo in your cluster and configure `mc` CLI tool. Instructions also [online](https://docs.min.io/docs/minio-client-quickstart-guide.html).\n",
    "\n",
    "-- Note: You can use your prefer remote storage server (google/ AWS etc.)\n",
    "\n",
    "#### Create a Bucket and store your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lasting-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[32;1mBucket created successfully `minio/language-models-multi/onnx-gpt2/1`.\u001b[0m\n",
      "...odel.onnx:  622.29 MiB / 622.29 MiB â”ƒâ–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â”ƒ 135.82 MiB/s 4s\u001b[0m\u001b[0m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m\u001b[m\u001b[32;1m"
     ]
    }
   ],
   "source": [
    "!mc mb minio/language-models-multi/onnx-gpt2/1 -p\n",
    "!mc cp ./model1/model.onnx minio/language-models-multi/onnx-gpt2-model1/1/\n",
    "!mc cp ./model2/model.onnx minio/language-models-multi/onnx-gpt2-model2/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-syracuse",
   "metadata": {},
   "source": [
    "### Run Seldon in your kubernetes cluster\n",
    "\n",
    "Follow the [Seldon-Core Setup notebook](https://docs.seldon.io/projects/seldon-core/en/latest/examples/seldon_core_setup.html) to Setup a cluster with Ambassador Ingress or Istio and install Seldon Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-outreach",
   "metadata": {},
   "source": [
    "### Deploy your model with Seldon pre-packaged Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "declared-crown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing secret.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile secret.yaml\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: seldon-init-container-secret\n",
    "type: Opaque\n",
    "stringData:\n",
    "  RCLONE_CONFIG_S3_TYPE: s3\n",
    "  RCLONE_CONFIG_S3_PROVIDER: minio\n",
    "  RCLONE_CONFIG_S3_ENV_AUTH: \"false\"\n",
    "  RCLONE_CONFIG_S3_ACCESS_KEY_ID: minioadmin\n",
    "  RCLONE_CONFIG_S3_SECRET_ACCESS_KEY: minioadmin\n",
    "  RCLONE_CONFIG_S3_ENDPOINT: http://minio.minio-system.svc.cluster.local:9000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beneficial-anime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gpt2-multi-deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile gpt2-multi-deploy.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1alpha2\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: gpt2-multi\n",
    "spec:\n",
    "  predictors:\n",
    "  - graph:\n",
    "      implementation: TRITON_SERVER\n",
    "      logger:\n",
    "        mode: all\n",
    "      modelUri: s3://language-models-multi\n",
    "      envSecretRefName: seldon-init-container-secret\n",
    "      name: gpt2-multi\n",
    "      type: MODEL\n",
    "    name: default\n",
    "    replicas: 1\n",
    "  protocol: kfserving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "subjective-involvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/seldon-init-container-secret configured\n",
      "seldondeployment.machinelearning.seldon.io/gpt2-multi created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f secret.yaml -n default\n",
    "!kubectl apply -f gpt2-multi-deploy.yaml -n default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "demanding-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment \"gpt2-multi-default-0-gpt2-multi\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "deployment \"gpt2-multi-default-0-gpt2-multi\" successfully rolled out\n"
     ]
    }
   ],
   "source": [
    "!kubectl rollout status deploy/$(kubectl get deploy -l seldon-deployment-id=gpt2-multi -o jsonpath='{.items[0].metadata.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-supervisor",
   "metadata": {},
   "source": [
    "#### Interact with the model: get model metadata (a \"test\" request to make sure our model is available and loaded correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "married-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"error\":\"Request for unknown model: 'gpt2-multi' is not found\"}"
     ]
    }
   ],
   "source": [
    "!curl -s http://localhost:32000/seldon/default/gpt2-multi/v2/models/gpt2-multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-resource",
   "metadata": {},
   "source": [
    "### Run prediction test: generate a sentence completion using GPT2 model  - Greedy approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "modified-termination",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/cc/seldon-core/examples/triton_gpt2/gpt2-two-models.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/seldon-core/examples/triton_gpt2/gpt2-two-models.ipynb#ch0000019vscode-remote?line=39'>40</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/seldon-core/examples/triton_gpt2/gpt2-two-models.ipynb#ch0000019vscode-remote?line=41'>42</a>\u001b[0m \u001b[39m# extract logits\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/seldon-core/examples/triton_gpt2/gpt2-two-models.ipynb#ch0000019vscode-remote?line=42'>43</a>\u001b[0m logits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(res[\u001b[39m\"\u001b[39;49m\u001b[39moutputs\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/seldon-core/examples/triton_gpt2/gpt2-two-models.ipynb#ch0000019vscode-remote?line=43'>44</a>\u001b[0m logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mreshape(res[\u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bchameleon/home/cc/seldon-core/examples/triton_gpt2/gpt2-two-models.ipynb#ch0000019vscode-remote?line=45'>46</a>\u001b[0m \u001b[39m# take the best next token probability of the last token of input ( greedy approach)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'outputs'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_text = \"I enjoy working in Seldon\"\n",
    "count = 0\n",
    "max_gen_len = 10\n",
    "gen_sentence = input_text\n",
    "while count < max_gen_len:\n",
    "    input_ids = tokenizer.encode(gen_sentence, return_tensors=\"tf\")\n",
    "    shape = input_ids.shape.as_list()\n",
    "    payload = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"input_ids\",\n",
    "                \"datatype\": \"INT32\",\n",
    "                \"shape\": shape,\n",
    "                \"data\": input_ids.numpy().tolist(),\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"attention_mask\",\n",
    "                \"datatype\": \"INT32\",\n",
    "                \"shape\": shape,\n",
    "                \"data\": np.ones(shape, dtype=np.int32).tolist(),\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    ret = requests.post(\n",
    "        \"http://localhost:32000/seldon/default/gpt2/v2/models/onnx-gpt2-model1/infer\",\n",
    "        json=payload,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        res = ret.json()\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # extract logits\n",
    "    logits = np.array(res[\"outputs\"][1][\"data\"])\n",
    "    logits = logits.reshape(res[\"outputs\"][1][\"shape\"])\n",
    "\n",
    "    # take the best next token probability of the last token of input ( greedy approach)\n",
    "    next_token = logits.argmax(axis=2)[0]\n",
    "    next_token_str = tokenizer.decode(\n",
    "        next_token[-1:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    ).strip()\n",
    "    gen_sentence += \" \" + next_token_str\n",
    "    count += 1\n",
    "\n",
    "print(f\"Input: {input_text}\\nOutput: {gen_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-status",
   "metadata": {},
   "source": [
    "### Run Load Test / Performance Test using vegeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-discovery",
   "metadata": {},
   "source": [
    "#### Install vegeta, for more details take a look in [vegeta](https://github.com/tsenart/vegeta#install) official documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/tsenart/vegeta/releases/download/v12.8.3/vegeta-12.8.3-linux-amd64.tar.gz\n",
    "!tar -zxvf vegeta-12.8.3-linux-amd64.tar.gz\n",
    "!chmod +x vegeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-lying",
   "metadata": {},
   "source": [
    "#### Generate vegeta [target file](https://github.com/tsenart/vegeta#-targets) contains \"post\" cmd with payload in the requiered structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "reliable-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from subprocess import PIPE, Popen, run\n",
    "\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "input_text = \"I enjoy working in Seldon\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "shape = input_ids.shape.as_list()\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        {\n",
    "            \"name\": \"input_ids\",\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"shape\": shape,\n",
    "            \"data\": input_ids.numpy().tolist(),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"attention_mask\",\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"shape\": shape,\n",
    "            \"data\": np.ones(shape, dtype=np.int32).tolist(),\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "cmd = {\n",
    "    \"method\": \"POST\",\n",
    "    \"header\": {\"Content-Type\": [\"application/json\"]},\n",
    "    \"url\": \"http://localhost:80/seldon/default/gpt2/v2/models/gpt2/infer\",\n",
    "    \"body\": base64.b64encode(bytes(json.dumps(payload), \"utf-8\")).decode(\"utf-8\"),\n",
    "}\n",
    "\n",
    "with open(\"vegeta_target.json\", mode=\"w\") as file:\n",
    "    json.dump(cmd, file)\n",
    "    file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vegeta attack -targets=vegeta_target.json -rate=1 -duration=60s -format=json | vegeta report -type=text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-suite",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pacific-collectible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seldondeployment.machinelearning.seldon.io \"gpt2\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f gpt2-deploy.yaml -n default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('central')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2465c4f56298bc06dbdad3e7519856d346ec0e9edf6ba2c905f0af711583810e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
